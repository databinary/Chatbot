{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chatbot Development\n",
    "#Import nltk library\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# GUI to download certain libraries\n",
    "nltk.download_gui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import interactive shell to provide multiple outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a sentence into lower case\n",
    "# Tokenize the sentence and extract individual words\n",
    "# Remove words that do not add meaning to the sentence\n",
    "sentence1 = 'The Big brown box jumped over a fox'\n",
    "def preprocess(sentence1):\n",
    "    sentence1= sentence1.lower()\n",
    "    tokenizer = RegexpTokenizer(\"\\w+\")\n",
    "    tokens = tokenizer.tokenize(sentence1)\n",
    "    filtered_words = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print preprocessed words\n",
    "preprocess_sentence = preprocess(sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = nltk.pos_tag(preprocess_sentence)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_tagged(sentences):\n",
    "    feature = []\n",
    "    for tagged_word in sentences:\n",
    "        word, tag = tagged_word\n",
    "        feature.append(word)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_tagged(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize the word\n",
    "lmtzr = WordNetLemmatizer()\n",
    "print(lmtzr.lemmatize('dollars'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming the words\n",
    "stemwords = ['stem', 'stems', 'stemming', 'foot', 'feeter']\n",
    "stemmer  = SnowballStemmer('english')\n",
    "[stemmer.stem(x) for x in stemwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting it all together\n",
    "def extract_features(text):\n",
    "    words = preprocess(text)\n",
    "    tags  = nltk.pos_tag(words)\n",
    "    extract_features = extra_tagged(tags)\n",
    "    stemmed_words = [stemmer.stem(x) for x in extract_features]\n",
    "    result = [lmtzr.lemmatize(x) for x in stemmed_words]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = 'he hurt is right foot while wearing a shoe on his lazy feet'\n",
    "words = extract_features(sentence2)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_feats(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the document\n",
    "def extract_feature_from_doc(data):\n",
    "    result = []\n",
    "    corpus = []\n",
    "    # the responses of the chatbot\n",
    "    answers = {}\n",
    "    for text, category, answer in data:\n",
    "        features = extract_features(text)\n",
    "        corpus.append(features)\n",
    "        result.append((word_feats(features), category))\n",
    "        answers[category] = answer\n",
    "    return (result, sum(corpus, []), answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(filename):\n",
    "    doc=os.path.join(filename)\n",
    "    with open(doc,'r') as content_file:\n",
    "        lines = csv.reader(content_file, delimiter='|')\n",
    "        data = [x for x in lines if len(x) == 3]\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'C:/Lenin Data Science/leaves.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_content(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data, corpus, answers = extract_feature_from_doc(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_data[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model using this features\n",
    "# split data into test and train\n",
    "split_ratio = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data, split_ratio):\n",
    "    random.shuffle(data)\n",
    "    data_length = len(data)\n",
    "    train_split = int(data_length * split_ratio)\n",
    "    return(data[:train_split], data[train_split:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = split_dataset(features_data, split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "np.save('training_data', train_data)\n",
    "np.save('test_data', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load('training_data.npy', allow_pickle=True)\n",
    "test_data = np.load('test_data.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree classifier\n",
    "def train_decision_tree(training_data, test_data):\n",
    "    classifier = nltk.classify.DecisionTreeClassifier.train(training_data, entropy_cutoff=0.6, support_cutoff=6)\n",
    "    classifier_name = type(classifier).__name__\n",
    "    training_set_accuracy = nltk.classify.accuracy(classifier, train_data)\n",
    "    print('training set accuracy', training_set_accuracy)\n",
    "    test_set_accuracy = nltk.classify.accuracy(classifier, test_data)\n",
    "    print('test set accuracy', test_set_accuracy)\n",
    "    return classifier, classifier_name, training_set_accuracy, test_set_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtclassifier, classifier_name, training_set_accuracy, test_set_accuracy = train_decision_tree(training_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifier_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes classifier\n",
    "def Naive_Bayes_Classifier(training_data, test_data):\n",
    "    classifier = nltk.NaiveBayesClassifier.train(training_data)\n",
    "    classifier_name = type(classifier).__name__\n",
    "    training_set_accuracy = nltk.classify.accuracy(classifier,training_data)\n",
    "    print('training set accuracy', training_set_accuracy)\n",
    "    test_set_accuracy = nltk.classify.accuracy(classifier,test_data)\n",
    "    print('test set accuracy', test_set_accuracy)\n",
    "    return classifier, classifier_name, training_set_accuracy, test_set_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbclassifier, classifier_name, training_set_accuracy, test_set_accuracy = Naive_Bayes_Classifier(training_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(nbclassifier.most_informative_features()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbclassifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reply(input_sentence):\n",
    "    category = dtclassifier.classify(word_feats(extract_features(input_sentence)))\n",
    "    return answers[category]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply(\"what leaves I have used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
